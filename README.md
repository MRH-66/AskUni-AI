AI Help Desk Chatbot

Overview

The AI Help Desk Chatbot is an intelligent assistant designed to help university students with their queries regarding university policies, student guidelines, and general academic-related inquiries. It uses web scraping, document embedding, and large language models (LLMs) to provide precise answers.

How It Works ğŸš€

Data Collection

Scrapes university web pages using Playwright and BeautifulSoup.

Extracts text data and stores it in structured files.

Document Processing & Embeddings

Loads university guidelines, student handbooks, and scraped web data.

Splits text into smaller chunks and creates vector embeddings using sentence-transformers/all-mpnet-base-v2.

Stores embeddings in ChromaDB for efficient retrieval.

Query Processing

User asks a question via Streamlit-based UI.

Retrieves the most relevant context using semantic search on ChromaDB.

Summarizes the retrieved context using BART-Large-CNN.

Generates a final answer using LaMini-GPT-1.5B.

Features âœ¨

âœ… Web Scraping: Extracts information from university websites dynamically.
âœ… AI-Powered Answering: Uses two LLMsâ€”BART for summarization & LaMini-GPT for response generation.
âœ… ChromaDB for Retrieval: Efficiently searches for relevant content using embeddings.
âœ… Fast & Scalable: Supports multiple queries and document sources.
âœ… User-Friendly Interface: Built with Streamlit for easy interaction.

Why Two LLMs? ğŸ¤–ğŸ¤

We use two specialized models instead of one for optimal performance:

BART-Large-CNN: A state-of-the-art text summarization model that refines long, retrieved context into concise summaries.

LaMini-GPT-1.5B: A text-generation model fine-tuned for answering academic queries in a coherent and contextually accurate way.

Using one model for both tasks would lead to less accurate and longer responses. The two-step approach ensures better context understanding and precise answers.

Tech Stack ğŸ› ï¸

Python (Primary language)

Playwright (Web Scraping)

BeautifulSoup (HTML Parsing)

LangChain (Document Processing & Retrieval)

Hugging Face Transformers (LLMs)

ChromaDB (Vector Database for retrieval)

Streamlit (Frontend for chatbot UI)

Installation Guide ğŸ› ï¸

1ï¸âƒ£ Clone the repository

  git clone https://github.com/your-repo/ai-help-desk-chatbot.git
  cd ai-help-desk-chatbot

2ï¸âƒ£ Install Requirements

  pip install -r requirements.txt

3ï¸âƒ£ Download the LLMs (Hugging Face Checkpoints)

git clone https://huggingface.co/facebook/bart-large-cnn
git clone https://huggingface.co/MBZUAI/LaMini-GPT-1.5B

âš ï¸ Ensure both folders (BART & LaMini-GPT) are in the project directory.

4ï¸âƒ£ Run the Web Scraper (Optional)

  python scrapping.py

5ï¸âƒ£ Generate Embeddings & Store in ChromaDB

  python embeddings.py

6ï¸âƒ£ Start the AI Chatbot

  streamlit run main.py

Folder Structure ğŸ“‚

ğŸ“¦ ai-help-desk-chatbot
 â”£ ğŸ“‚ bart-large-cnn  # Downloaded LLM
 â”£ ğŸ“‚ LaMini-GPT-1.5B  # Downloaded LLM
 â”£ ğŸ“‚ chroma_db  # Vector database storage
 â”£ ğŸ“‚ Data  # Scraped data and documents
 â”£ ğŸ“‚ NoteBooks  # Jupyter Notebooks for preprocessing and conversions
 â”ƒ â”£ ğŸ“œ clean_scraping_data.ipynb  # Cleans and converts scraped text into a structured PDF
 â”ƒ â”£ ğŸ“œ Data_Preprocessing.ipynb  # Processes Excel student data
 â”ƒ â”— ğŸ“œ CSV-to-PDF.ipynb  # Converts CSV files into formatted PDFs
 â”£ ğŸ“œ scrapping.py  # Web scraper
 â”£ ğŸ“œ embeddings.py  # Embedding generation
 â”£ ğŸ“œ utils.py  # Context retrieval, summarization, answering
 â”£ ğŸ“œ main.py  # Streamlit UI for chatbot
 â”£ ğŸ“œ requirements.txt  # Dependencies
 â”— ğŸ“œ README.md  # Project Documentation

Jupyter Notebooks (Data Preprocessing & Conversion) ğŸ“

The notebooks folder contains useful Jupyter notebooks for cleaning and converting data before usage in the chatbot:

1ï¸âƒ£ clean_scraping_data.ipynb

Reads raw scraped text from raw_scraped_data.txt.

Cleans unwanted HTML tags, special characters, and formatting issues.

Converts the cleaned text into a structured PDF (Clean_Scrape_data.pdf).

Useful when using scrapped data for document embeddings.

How to Use:

Place your raw scraped text inside data/raw_scraped_data.txt.

Run the notebook to generate Clean_Scrape_data.pdf.

Use the cleaned PDF for embedding generation.

2ï¸âƒ£ Data_Preprocessing.ipynb

Processes Excel files containing student data.

Cleans and structures the data for further analysis or AI model usage.

Useful if the chatbot requires student-specific insights.

How to Use:

Place your Excel file in the data/ folder.

Open and run Data_Preprocessing.ipynb.

The cleaned output will be available for further processing.

3ï¸âƒ£ CSV-to-PDF.ipynb

Converts structured CSV data into a formatted PDF report.

Useful for generating student reports or summarizing scraped data.

How to Use:

Ensure your CSV file is inside data/.

Run CSV-to-PDF.ipynb.

The generated PDF will be available for further use.

Troubleshooting ğŸ”§

âŒ Issue: "ModuleNotFoundError: No module named 'playwright'"
âœ… Fix:

pip install playwright
playwright install

âŒ Issue: "Device ran out of memory (OOM Error)"
âœ… Fix:

Reduce chunk_size in embeddings.py.

Use device='cpu' in models (if running on a low-GPU machine).

âŒ Issue: "No response from chatbot"
âœ… Fix:

Ensure both LLMs are downloaded and placed in the project directory.

Run python embeddings.py again before starting main.py.

Future Improvements ğŸš€

ğŸ”¹ Add support for more university departments (expandable knowledge base)
ğŸ”¹ Improve multi-turn conversations
ğŸ”¹ Implement fine-tuned retrieval models for better accuracy
ğŸ”¹ Support voice-based queries

Contributors ğŸ‘¨â€ğŸ’»

Rehan Hanif

ğŸ“§ Contact: rehan.hanif2004@gmail.com

ğŸ“Œ Ready to Try? Start the chatbot and explore AI-powered university assistance! ğŸ“ğŸ¤–

